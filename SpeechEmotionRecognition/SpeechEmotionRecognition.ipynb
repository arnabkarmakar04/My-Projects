{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_dir = Path(\"TESS_Data\")\n",
    "tess_files = list(tess_dir.glob('**/*.wav'))\n",
    "tess_labels = [path.parent.name for path in tess_files]\n",
    "audio_df = pd.DataFrame({'audio_file': tess_files, 'emotion': tess_labels}).sample(frac=1, random_state= 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total files:\", len(audio_df))\n",
    "print(audio_df['emotion'].value_counts())\n",
    "print(\"Emotions:\", \", \".join(audio_df['emotion'][:5].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Audio Processing Functions with Some Additional Tunings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    try:\n",
    "        data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "        data = librosa.util.normalize(data)\n",
    "        return data, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data):\n",
    "    noise_value = 0.015 * np.random.uniform() * np.amax(data)\n",
    "    return data + noise_value * np.random.normal(size=data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_audio(data, rate=0.9):\n",
    "    return librosa.effects.time_stretch(data, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_pitch(data, sampling_rate, pitch_factor=3):\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Feature Extraction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction as MFCC sequences\n",
    "def extract_features(data, sample_rate, n_mfcc=40, max_pad_len=100):\n",
    "    # Extract MFCC (as sequence)\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    mfcc = mfcc.T  # Shape: (timesteps, features)\n",
    "    if mfcc.shape[0] < max_pad_len:\n",
    "        pad_width = max_pad_len - mfcc.shape[0]\n",
    "        mfcc = np.pad(mfcc, ((0,pad_width),(0,0)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:max_pad_len, :]\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_features(path):\n",
    "    data, sample_rate = load_audio(path)\n",
    "    if data is None:\n",
    "        return np.array([])\n",
    "    result = [extract_features(data, sample_rate)]\n",
    "    noisy_data = add_noise(data)\n",
    "    result.append(extract_features(noisy_data, sample_rate))\n",
    "    stretched_pitch = change_pitch(stretch_audio(data), sample_rate)\n",
    "    result.append(extract_features(stretched_pitch, sample_rate))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = [], []\n",
    "for path, emotion in zip(audio_df.audio_file, audio_df.emotion):\n",
    "    features = export_features(path)\n",
    "    if features.size > 0:\n",
    "        for seq in features:\n",
    "            X_data.append(seq)\n",
    "            y_data.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data).reshape(-1, 1)\n",
    "\n",
    "print(\"Feature shape:\", X_data.shape)\n",
    "print(\"Labels shape:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Checking Some Audio Samples With Above Tunings & Plotting Them***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, speech = read(audio_df['audio_file'][2342])\n",
    "print(audio_df['emotion'][2342])\n",
    "Audio(speech, rate=rate, autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, speech = read(audio_df['audio_file'][20])\n",
    "print(audio_df['emotion'][20])\n",
    "Audio(speech, rate=rate, autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "audio_speech,rate = librosa.load(audio_df['audio_file'][120])\n",
    "print(audio_df['emotion'][120])\n",
    "librosa.display.waveshow(audio_speech, sr=rate, color = 'orange')\n",
    "Audio(audio_speech, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "audio_speech,rate = librosa.load(audio_df['audio_file'][10])\n",
    "print(audio_df['emotion'][10])\n",
    "librosa.display.waveshow(audio_speech, sr=rate, color = 'green')\n",
    "Audio(audio_speech, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "plt.title(\"Tune 1: Voice With Some Noise\")\n",
    "audio_speech,sample_rate = librosa.load(audio_df['audio_file'][2000])\n",
    "print(audio_df['emotion'][2000])\n",
    "noise_injection = add_noise(audio_speech)\n",
    "librosa.display.waveshow(noise_injection, sr=sample_rate)\n",
    "Audio(noise_injection, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "plt.title(\"Tune 2: Streched Voice\")\n",
    "audio_speech,sample_rate = librosa.load(audio_df['audio_file'][2000])\n",
    "print(audio_df['emotion'][2000])\n",
    "stretching_audio = stretch_audio(audio_speech)\n",
    "librosa.display.waveshow(stretching_audio, sr=sample_rate, color='red')\n",
    "Audio(stretching_audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Spectrogram of a Audio Sample***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(13,6))\n",
    "audio_speech, rate = librosa.load(audio_df['audio_file'][2000])\n",
    "stft_audio = librosa.stft(audio_speech)\n",
    "Db_audio = librosa.amplitude_to_db(abs(stft_audio))\n",
    "librosa.display.specshow(Db_audio, sr=rate, x_axis='time', y_axis='hz')\n",
    "plt.title('Spectrogram')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Visualization of MFCCs of a Audio Sample***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file, sr = librosa.load(audio_df['audio_file'][2000])\n",
    "mfccs = librosa.feature.mfcc(y=audio_file, sr=sr, n_mfcc=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap='cool')\n",
    "plt.title('Mel-Frequency Cepstral Coefficients (MFCCs)')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_label = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "Y = encoder_label.fit_transform(y_data)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = X_train.shape[1]\n",
    "num_features = X_train.shape[2]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_flat = X_train.reshape(-1, num_features)\n",
    "X_test_flat = X_test.reshape(-1, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_flat.shape)\n",
    "print(X_test_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Creation (LSTM)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping for LSTM\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(-1, num_timesteps, num_features)\n",
    "X_test_scaled = scaler.transform(X_test_flat).reshape(-1, num_timesteps, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer= optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = create_lstm_model((num_timesteps, num_features), Y.shape[1])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint('speech-emotion-recognition.keras', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Visualization of Accuracy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', color='red', marker='o', linestyle='-')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='blue', marker='o', linestyle='-')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='red', marker='o', linestyle='-')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='blue', marker='o', linestyle='-')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('speech-emotion-recognition.keras')\n",
    "results = model.evaluate(X_test_scaled, y_test)\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict(X_test_scaled)\n",
    "y_pred = encoder_label.inverse_transform(test_prediction)\n",
    "y_test_inv = encoder_label.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted labels:\", y_pred[:10])\n",
    "print(\"Actual labels   :\", y_test_inv[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Confusion Matrix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_prediction, axis=1))\n",
    "plt.figure(figsize=(13, 6))\n",
    "sns.heatmap(conf_matrix, linecolor='white', cmap='Blues', annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Classification Report***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_inv, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
